scrapy框架

- 什么是框架？
    - 就是一个集成了很多功能并且具有很强通用性的一个项目框架。

- 如何学习框架？
     - 专门学习框架封装的各种功能的详细用法。

- 什么是scrapy?
    - 爬虫中封装好的一个明星框架。功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式...

- scrapy框架的基本使用
    - 环境安装
        - mac/linux: pip install scrapy
        - windows:
            - pip install wheel (pip install wheel -i https://pypi.tuna.tsinghua.edu.cn/simple)
            - 下载twisted下载地址为https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
            - 安装twisted: pip install Twisted-17. 1.0-cp36-cp36m-win-amd64 whl
            - pip install pywin32
            - pip install scrapy
            测试：在终端里录入 scrap指令，没有报错即表示安装成功！
    - 创建一个工程： scrapy startproject xxxPro
    - cd xxxPro
    - 在spiders子目录中创建一个爬虫文件
        - scrapy genspider spiderName www.xxx.com
    - 执行工程：
        - scrapy crawl spiderName

- scrapy数据解析

- scrapy持久化存储
    - 基于终端指令：
        - 要求：只可以将parse方法的返回值存储到本地的文本文件中
        - 持久化存储对应的文本文件的类型只可以是：'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle'中的一种
        - 指令：scrapy crawl qiubai -o ./qiubai.csv
              - scrapy crawl xxx -o filePath
        -优点：简洁高效便捷
        -缺点：局限性比较强（数据只可以存储到指定后缀的文本文件中）

    - 基于管道：
        - 编码流程：
            - 数据解析
            - 在item类中定义相关的属性
            - 将解析的数据封装存储到item类型的对象
            - 将item类型的对象提交给管道进行持久化存储的操作
            - 在管道类的process_item中要将其接收到的item对象中存储的数据进行持久化存储操作
            - 在配置文件中开启管道
        - 优点：
            - 通用性强。

    - 面试题：将爬取到的数据一份存储到本地，一份存储到数据库，如何实现？
        - 管道文件中的一个管道类对应的是将数据存储到一种平台
        - 爬虫文件提交的item只会给管道文件中第一个被执行的管道类接收
        - process_item中的return item表示将item传递给下一个即将被执行的管道类

- 基于Spider的全站数据爬取
    - 就是将网站中某板块下的全部页码对应的页面数据进行爬取
    - 需求：爬取校花网中的照片的名称
    - 实现方式：
        - 将所有页面的url添加到start_urls列表（不推荐）
        - 自行手动进行请求发送（推荐）
            - 手动请求发送：
                - yield scrapy.Request(url,callback):callback专门用作于数据解析


- 五大核心组件
    - 引擎(Scrapy)
        用来处理整个系统的数据流处理，触发事务（框架核心）
    - 调度器(Scheduler)
        用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列，
        由它来决定下一个要抓取的网址是什么，同时去除重复的网址
    - 下载器(Downloader)
        用于下载网页内容，并将网页内容返回给蜘蛛(Scrap下载器是建立在twisted这个高效的异步模型上的)
    - 爬虫(Spiders)
        爬虫是主要干活的，用于从特定的网页中提取自己需要的信息即所谓的实体(Item)。用户也可以从中提取出链接让Scraps继续抓取下一个页面
    - 项目管道(Pipeline)
        负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，
        并经过几个特定的次序处理数据


- 请求传参
    - 使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）
    - 需求：爬取boss直聘的岗位名称和岗位描述


- 图片数据爬取之ImagePipeline
    - 基于scrapy爬取字符串类型的数据和爬取图片的数据区别？
        - 字符串： 只需要基于xpath进行解析且提交管道进行持久化存储
        - 图片：xpath解析出图片src属性值。单独的对图片地址发起请求获取图片二进制类型的数据。

    - ImagePipeline:
        - 只需要将img的src的属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制类型的数据，且还会帮我们进行持久化存储。
    - 需求：爬取站长素材中的高清图片
    - 使用流程：
        - 数据解析（图片的地址）
        - 将存储图片地址的主tem提交到制定的管道类
        - 在管道文件中自定制一个基于 Imagespipelinel的一个管道类
            - get_media_request
            - file_path
            - item_completed
        - 在配置文件中：
            - 指定图片存储的目录： IIMAGES_STORE="/imgs_bobQ
            - 指定开启的管道：自定制的管道类
